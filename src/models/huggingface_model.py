"""HuggingFace model implementation."""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Optional
import logging
import re
import os
import time

from .base import BaseModel

# Try to import huggingface_hub for download progress tracking
try:
    from huggingface_hub import hf_hub_download
    from huggingface_hub.utils import tqdm as hf_tqdm
    HAS_HF_HUB = True
except ImportError:
    HAS_HF_HUB = False

# Try to import tqdm for progress bars
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False


class DownloadProgressCallback:
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏."""
    
    def __init__(self, logger):
        self.logger = logger
        self.current_file = None
        self.file_start_time = None
        self.last_log_time = 0
        self.log_interval = 2.0  # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫—É–Ω–¥—ã
    
    def __call__(self, chunk_size, total_size=None):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞."""
        import time
        current_time = time.time()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –ª–æ–≥–∏
        if current_time - self.last_log_time >= self.log_interval:
            if total_size:
                downloaded_mb = (chunk_size * 100) / (1024 * 1024) if chunk_size else 0
                total_mb = total_size / (1024 * 1024)
                percent = (chunk_size / total_size * 100) if total_size > 0 else 0
                self.logger.info(f"   üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {downloaded_mb:.1f} MB / {total_mb:.1f} MB ({percent:.1f}%)")
            else:
                downloaded_mb = chunk_size / (1024 * 1024) if chunk_size else 0
                self.logger.info(f"   üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {downloaded_mb:.1f} MB")
            
            self.last_log_time = current_time


# Monkey-patch –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ torch.is_autocast_enabled()
# –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Nanbeige) –≤—ã–∑—ã–≤–∞—é—Ç torch.is_autocast_enabled(device),
# –Ω–æ –≤ PyTorch 2.2+ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
_original_is_autocast_enabled = torch.is_autocast_enabled
def _patched_is_autocast_enabled(*args, **kwargs):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è torch.is_autocast_enabled, –∏–≥–Ω–æ—Ä–∏—Ä—É—é—â–∞—è –∞—Ä–≥—É–º–µ–Ω—Ç—ã."""
    return _original_is_autocast_enabled()
torch.is_autocast_enabled = _patched_is_autocast_enabled


class HuggingFaceModel(BaseModel):
    """HuggingFace transformer model implementation."""
    
    def __init__(self, config):
        """Initialize HuggingFace model.
        
        Args:
            config: Model configuration containing:
                - model_path: HuggingFace model name or path
                - max_length: Maximum sequence length
                - temperature: Sampling temperature
                - top_p: Nucleus sampling parameter
                - device: Device to use (cuda/cpu)
                - use_flash_attention: Whether to use flash attention
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.last_prompt = None  # Store last used prompt for logging
        
        # Extract config parameters
        self.model_path = config.get('model_path', 'gpt2')
        self.max_length = config.get('max_length', 512)
        self.temperature = config.get('temperature', 0.7)
        self.top_p = config.get('top_p', 0.9)
        self.repetition_penalty = config.get('repetition_penalty', 1.1)  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
        self.max_new_tokens = config.get('max_new_tokens', 150)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        
        # Device configuration
        device_config = config.get('device', 'cuda')
        if device_config == 'cuda' and torch.cuda.is_available():
            self.device = torch.device('cuda')
            self.logger.info(f"üéØ Using GPU: {torch.cuda.get_device_name(0)}")
        else:
            self.device = torch.device('cpu')
            self.logger.info("üíª Using CPU")
        
        # Load model and tokenizer
        self.logger.info(f"üì¶ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {self.model_path}")
        
        try:
            # Check if model is cached
            cache_dir = os.environ.get('HF_HOME') or os.environ.get('TRANSFORMERS_CACHE') or os.path.expanduser('~/.cache/huggingface')
            self.logger.info(f"üíæ –ö—ç—à –º–æ–¥–µ–ª–µ–π: {cache_dir}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω HuggingFace –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
            hf_token = config.get('hf_token') or os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_HUB_TOKEN')
            if hf_token:
                self.logger.info("üîë –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è HuggingFace token –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏")
            
            token_kwargs = {'trust_remote_code': True}
            if hf_token:
                token_kwargs['token'] = hf_token
            
            # Load tokenizer
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...")
            tokenizer_start_time = time.time()
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º use_fast=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ tokenizers
            # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ ModelWrapper, —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    use_fast=True,
                    **token_kwargs
                )
                self.logger.info("‚úÖ –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º)")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {e}")
                self.logger.info("üîÑ –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    use_fast=False,
                    **token_kwargs
                )
                self.logger.info("‚úÖ –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (–º–µ–¥–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º)")
            
            tokenizer_time = time.time() - tokenizer_start_time
            self.logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞: {tokenizer_time:.2f} —Å–µ–∫")
            
            # Set padding token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Load model
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è)...")
            model_start_time = time.time()
            
            model_kwargs = {
                'trust_remote_code': True,
                'torch_dtype': torch.float16 if self.device.type == 'cuda' else torch.float32,
            }
            if hf_token:
                model_kwargs['token'] = hf_token
            
            # Add flash attention if requested and available
            if config.get('use_flash_attention', False):
                model_kwargs['attn_implementation'] = 'flash_attention_2'
                self.logger.info("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Flash Attention 2")
            
            # Log download progress
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face Hub...")
            
            # –í–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã Hugging Face Hub
            # –û–Ω–∏ –±—É–¥—É—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –≤ stderr –∏ –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å—Å—è run_batch_experiments.py
            os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –∫—ç—à—É –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            from pathlib import Path
            model_cache_name = self.model_path.replace('/', '--')
            model_cache_path = Path(cache_dir) / 'models' / f'models--{model_cache_name}'
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ
            if model_cache_path.exists():
                self.logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ: {model_cache_path}")
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤ –≤ –∫—ç—à–µ
                total_size = sum(f.stat().st_size for f in model_cache_path.rglob('*') if f.is_file())
                total_size_gb = total_size / (1024 ** 3)
                self.logger.info(f"   üíæ –†–∞–∑–º–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {total_size_gb:.2f} GB")
            else:
                self.logger.info(f"   ‚è≥ –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞...")
                self.logger.info(f"   üì• –ü—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏ –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –≤ –ª–æ–≥–∞—Ö –Ω–∏–∂–µ (stderr)")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )
            
            # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤
            if model_cache_path.exists():
                total_size = sum(f.stat().st_size for f in model_cache_path.rglob('*') if f.is_file())
                total_size_gb = total_size / (1024 ** 3)
                self.logger.info(f"   ‚úÖ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, —Ä–∞–∑–º–µ—Ä: {total_size_gb:.2f} GB")
            
            model_load_time = time.time() - model_start_time
            self.logger.info(f"‚úÖ –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {model_load_time:.2f} —Å–µ–∫")
            
            # Move model to device
            self.logger.info(f"üöÄ –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}...")
            device_start_time = time.time()
            self.model.to(self.device)
            self.model.eval()
            device_time = time.time() - device_start_time
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ {self.device} –∑–∞ {device_time:.2f} —Å–µ–∫")
            
            total_time = time.time() - tokenizer_start_time
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {total_time:.2f} —Å–µ–∫")
            
            # Log model info
            num_params = sum(p.numel() for p in self.model.parameters())
            self.logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {num_params:,}")
            
            # Log model size
            model_size_gb = self.get_model_size() / (1024 ** 3)
            self.logger.info(f"üíæ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏: {model_size_gb:.2f} GB")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error loading model: {e}")
            raise
    
    def generate(self, prompt: str, context: Optional[List[str]] = None, prompt_template: Optional[str] = None) -> str:
        """Generate answer for the given prompt.
        
        Args:
            prompt: Input question/prompt
            context: Optional context (list of strings)
            prompt_template: Optional custom prompt template (defaults to built-in templates)
            
        Returns:
            Generated answer text
        """
        # Check if this is a Qwen3 model (from unsloth) that needs chat template with thinking disabled
        is_qwen3 = 'Qwen3' in self.model_path or 'qwen3' in self.model_path.lower()
        use_chat_template = is_qwen3 and hasattr(self.tokenizer, 'apply_chat_template')
        
        if use_chat_template:
            # Use chat template for Qwen3 models with thinking disabled
            messages = []
            if context and len(context) > 0:
                context_str = "\n".join(context)
                messages.append({"role": "system", "content": f"Context: {context_str}"})
            
            if prompt_template:
                # Use custom prompt template but format it as a user message
                if context and len(context) > 0:
                    context_str = "\n".join(context)
                    user_content = prompt_template.replace("{context}", context_str).replace("{question}", prompt)
                else:
                    user_content = prompt_template.replace("{question}", prompt)
            else:
                user_content = prompt
            
            messages.append({"role": "user", "content": user_content})
            
            # Apply chat template with thinking disabled for Qwen3
            try:
                full_prompt = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=False  # Explicitly disable thinking mode
                )
                self.logger.debug("Using Qwen3 chat template with thinking disabled")
            except Exception as e:
                self.logger.warning(f"Failed to apply chat template: {e}, falling back to simple format")
                use_chat_template = False
        
        if not use_chat_template:
            # Format input using prompt template or default behavior
            if prompt_template:
                # Use custom prompt template from config
                if context and len(context) > 0:
                    context_str = "\n".join(context)
                    full_prompt = prompt_template.replace("{context}", context_str).replace("{question}", prompt)
                else:
                    full_prompt = prompt_template.replace("{question}", prompt)
            else:
                # Fallback to original behavior
                if context and len(context) > 0:
                    context_str = "\n".join(context)
                    full_prompt = f"Context: {context_str}\n\nQuestion: {prompt}\nAnswer:"
                else:
                    full_prompt = f"Question: {prompt}\nAnswer:"
        
        # Store last prompt for logging
        self.last_prompt = full_prompt
        
        try:
            # Tokenize input
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_length
            ).to(self.device)
            
            # Generate
            with torch.no_grad():
                # –î–ª—è –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                use_sampling = self.temperature > 0.1
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                generate_kwargs = dict(inputs)  # –ö–æ–ø–∏—Ä—É–µ–º inputs
                generate_kwargs.update({
                    'max_new_tokens': self.max_new_tokens,
                    'min_new_tokens': 1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                    'pad_token_id': self.tokenizer.pad_token_id,
                    'eos_token_id': self.tokenizer.eos_token_id,
                    'repetition_penalty': self.repetition_penalty,
                    'no_repeat_ngram_size': 2,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –±–∏–≥—Ä–∞–º–º
                })
                
                if use_sampling:
                    # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã > 0.1
                    generate_kwargs.update({
                        'temperature': self.temperature,
                        'top_p': self.top_p,
                        'do_sample': True,
                    })
                else:
                    # Greedy decoding –¥–ª—è –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
                    generate_kwargs['do_sample'] = False
                
                # –ü–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ autocast
                # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Nanbeige) –∏–º–µ—é—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.2+
                # –≥–¥–µ torch.is_autocast_enabled() –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã
                try:
                    outputs = self.model.generate(**generate_kwargs)
                except (TypeError, AttributeError) as e:
                    error_msg = str(e)
                    if "is_autocast_enabled() takes no arguments" in error_msg or "is_autocast_enabled" in error_msg:
                        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–±–ª–µ–º–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ autocast
                        # –û—Ç–∫–ª—é—á–∞–µ–º autocast —è–≤–Ω–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
                        self.logger.warning(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ autocast, –æ—Ç–∫–ª—é—á–∞—é autocast –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º torch.cuda.amp.autocast —Å enabled=False –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è autocast
                        with torch.cuda.amp.autocast(enabled=False):
                            outputs = self.model.generate(**generate_kwargs)
                    else:
                        # –ü–µ—Ä–µ–¥–∞–µ–º –æ—à–∏–±–∫—É –¥–∞–ª—å—à–µ, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞ autocast
                        raise
            
            # Decode output
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the answer part
            if use_chat_template:
                # For chat template format, extract assistant response
                # Qwen3 chat template typically has assistant role markers
                if "<|im_start|>assistant" in generated_text:
                    answer = generated_text.split("<|im_start|>assistant")[-1].strip()
                    # Remove any remaining role markers
                    answer = re.sub(r'<\|im_end\|>.*$', '', answer, flags=re.DOTALL).strip()
                elif "assistant" in generated_text.lower():
                    # Fallback: try to find assistant response
                    parts = re.split(r'assistant\s*:?\s*', generated_text, flags=re.IGNORECASE)
                    if len(parts) > 1:
                        answer = parts[-1].strip()
                    else:
                        answer = generated_text[len(full_prompt):].strip()
                else:
                    # If no assistant marker found, extract text after prompt
                    answer = generated_text[len(full_prompt):].strip()
            else:
                # Original extraction logic for non-chat-template format
                if "Answer:" in generated_text:
                    answer = generated_text.split("Answer:")[-1].strip()
                else:
                    answer = generated_text[len(full_prompt):].strip()
            
            # Clean up the answer - —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            # –ù–ï –æ–±—Ä–µ–∑–∞–µ–º –ø–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ, —Ç–∞–∫ –∫–∞–∫ –æ—Ç–≤–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º
            answer = answer.strip()
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            answer = re.sub(r'\s+', ' ', answer).strip()
            
            # Remove thinking-related tags (Qwen3 thinking mode output)
            # Remove <think>...</think> tags and their content (including empty tags)
            answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.IGNORECASE | re.DOTALL)
            # Also handle self-closing tags
            answer = re.sub(r'<think\s*/>', '', answer, flags=re.IGNORECASE)
            # Remove any remaining <think> or </think> tags
            answer = re.sub(r'</?think>', '', answer, flags=re.IGNORECASE)
            
            # Remove thinking-related patterns if they still appear (fallback cleanup)
            thinking_patterns = [
                r'Step-by-Step Explanation:.*?(?=\w)',
                r'Let me think.*?(?=\w)',
                r'First,.*?(?=\w)',
            ]
            for pattern in thinking_patterns:
                answer = re.sub(pattern, '', answer, flags=re.IGNORECASE | re.DOTALL)
            
            # Final cleanup - remove extra spaces that might have been created
            answer = re.sub(r'\s+', ' ', answer).strip()
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)
            if not hasattr(self, '_debug_log_count'):
                self._debug_log_count = 0
            if self._debug_log_count < 3:
                self.logger.debug(f"–ü–æ–ª–Ω—ã–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {generated_text}")
                self.logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {answer}")
                self._debug_log_count += 1
            
            return answer if answer else "No answer generated"
            
        except Exception as e:
            self.logger.error(f"Error generating answer: {e}")
            return "Error generating answer"
    
    def get_model_size(self) -> int:
        """Get model size in bytes.
        
        Returns:
            Model size in bytes
        """
        return sum(p.nelement() * p.element_size() for p in self.model.parameters())
    
    def get_prompt_template(self, with_context: bool = False) -> str:
        """Get prompt template used by the model.
        
        Args:
            with_context: Whether to show template with or without context
            
        Returns:
            Prompt template string
        """
        if with_context:
            return "Context: {context}\n\nQuestion: {question}\nAnswer:"
        else:
            return "Question: {question}\nAnswer:"

