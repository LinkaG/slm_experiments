from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import logging
from pathlib import Path
from statistics import mean
import json
import time
from clearml import Task, Logger
from omegaconf import OmegaConf
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

from ..models.base import BaseModel
from ..retrievers.base import BaseRetriever
from ..data.base import BaseDataset, DatasetItem
from .metrics import TokenRecallCalculator
from ..utils.memory_tracker import MemoryTracker
from ..utils.predictions_tracker import PredictionsTracker
from ..utils.logger_wrapper import LoggerWrapper
from ..utils.local_logger import LocalLogger
from ..utils.clearml_config import (
    setup_clearml_environment, 
    create_clearml_task, 
    get_clearml_logger,
    log_experiment_config,
    log_predictions_to_clearml,
    log_metrics_to_clearml
)

@dataclass
class ExperimentConfig:
    """Configuration for an experiment run."""
    name: str
    model_config: Dict[str, Any]
    retriever_config: Dict[str, Any]
    dataset_config: Dict[str, Any]
    metrics_config: Dict[str, Any]
    output_dir: Path
    max_samples: Optional[int] = None
    use_retriever: bool = False
    context_type: str = "none"
    prompt_template: str = "Question: {question}\nAnswer:"  # default prompt
    model: Optional[Any] = None

class ExperimentRunner:
    """Main class for running experiments."""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º metric_calculator –ø–æ–∑–∂–µ, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        # —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ tokenizer, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
        self.metric_calculator = None
        self.memory_tracker = MemoryTracker(Path(config.output_dir))
        self.predictions_tracker = PredictionsTracker(Path(config.output_dir))
        
    def setup_experiment(self, use_clearml: bool = True):
        """Initialize ClearML, create directories, etc."""
        self.use_clearml = use_clearml  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–∞—Ö
        if use_clearml:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º ClearML —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º .env —Ñ–∞–π–ª–∞
            setup_clearml_environment()
            
            # –°–æ–∑–¥–∞–µ–º ClearML –∑–∞–¥–∞—á—É
            self.task = create_clearml_task(
                project_name="slm-experiments",
                task_name=self.config.name,
                tags=[self.config.model_config.get('name', 'unknown'), 
                      self.config.dataset_config.get('name', 'unknown'),
                      self.config.context_type]
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º OmegaConf –æ–±—ä–µ–∫—Ç—ã –≤ –æ–±—ã—á–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è ClearML
            config_dict = {
                "model": self.config.model_config,
                "retriever": self.config.retriever_config,
                "dataset": self.config.dataset_config,
                "metrics": self.config.metrics_config,
                "experiment": {
                    "name": self.config.name,
                    "max_samples": self.config.max_samples,
                    "use_retriever": self.config.use_retriever,
                    "context_type": self.config.context_type
                },
                "prompt": {
                    "template": self.config.prompt_template
                }
            }
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Å—å —Å–ª–æ–≤–∞—Ä—å —Ü–µ–ª–∏–∫–æ–º
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å (–µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã —É–∂–µ dict, —Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
            try:
                config_plain = OmegaConf.to_container(config_dict, resolve=True)
            except (ValueError, TypeError):
                # –ï—Å–ª–∏ config_dict —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—ã—á–Ω—ã–µ dict, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
                config_plain = config_dict
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            self.task.connect(config_plain)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            clearml_logger = get_clearml_logger()
            self.logger = LoggerWrapper(clearml_logger)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            full_config = {
                "model": self.config.model_config,
                "retriever": self.config.retriever_config,
                "dataset": self.config.dataset_config,
                "metrics": self.config.metrics_config,
                "experiment": {
                    "name": self.config.name,
                    "output_dir": str(self.config.output_dir),
                    "max_samples": self.config.max_samples,
                    "use_retriever": self.config.use_retriever,
                    "context_type": self.config.context_type
                },
                "prompt": {
                    "template": self.config.prompt_template
                }
            }
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ–±—ã—á–Ω—ã–µ Python —Ç–∏–ø—ã
            try:
                full_config_plain = OmegaConf.to_container(full_config, resolve=True)
            except (ValueError, TypeError):
                # –ï—Å–ª–∏ full_config —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—ã—á–Ω—ã–µ dict, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
                full_config_plain = full_config
            log_experiment_config(self.logger, full_config_plain)
        else:
            # –†–µ–∂–∏–º –±–µ–∑ ClearML - –∏—Å–ø–æ–ª—å–∑—É–µ–º LocalLogger
            self.task = None
            self.local_logger = LocalLogger(self.config.output_dir)
            python_logger = logging.getLogger(__name__)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º LocalLogger –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π logger —á–µ—Ä–µ–∑ wrapper
            self.logger = LoggerWrapper(self.local_logger)
            self.logger.info("üöÄ –ù–∞—á–∞–ª–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–±–µ–∑ ClearML)")
            self.logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {self.config.output_dir}")
            self.logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.config.model_config.get('name', 'unknown')}")
            self.logger.info(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {self.config.dataset_config.get('name', 'unknown')}")
            self.logger.info(f"üîç –†–µ–∂–∏–º: {self.config.context_type}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
            full_config = {
                "model": self.config.model_config,
                "retriever": self.config.retriever_config,
                "dataset": self.config.dataset_config,
                "metrics": self.config.metrics_config,
                "experiment": {
                    "name": self.config.name,
                    "output_dir": str(self.config.output_dir),
                    "max_samples": self.config.max_samples,
                    "use_retriever": self.config.use_retriever,
                    "context_type": self.config.context_type
                },
                "prompt": {
                    "template": self.config.prompt_template
                }
            }
            try:
                full_config_plain = OmegaConf.to_container(full_config, resolve=True)
            except (ValueError, TypeError):
                full_config_plain = full_config
            self.local_logger.config = full_config_plain
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        
    def run(self, model: BaseModel, retriever: BaseRetriever, dataset: BaseDataset, use_clearml: bool = True):
        """Run the experiment."""
        self.setup_experiment(use_clearml=use_clearml)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º metric_calculator —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª–∏
        # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ recall - –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        if hasattr(model, 'tokenizer') and model.tokenizer is not None:
            self.metric_calculator = TokenRecallCalculator(tokenizer=model.tokenizer)
            self.logger.info("‚úÖ TokenRecallCalculator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª–∏")
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –º–æ–¥–µ–ª–∏
            model_path = self.config.model_config.get('model_path', 'bert-base-uncased')
            self.metric_calculator = TokenRecallCalculator(tokenizer_name=model_path)
            self.logger.warning(f"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {model_path}")
        
        # Initial memory state
        self.memory_tracker.log_memory("system", "experiment_start")
        
        # Log prompt template from config
        if hasattr(self.logger, 'report_text'):
            self.logger.report_text("üìù –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞:")
            self.logger.report_text("")
            self.logger.report_text("```")
            self.logger.report_text(self.config.prompt_template)
            self.logger.report_text("```")
        else:
            self.logger.info(f"Prompt template: {self.config.prompt_template}")
        
        # Log basic info as single values (–Ω–µ —Å–æ–∑–¥–∞—é—Ç –≥—Ä–∞—Ñ–∏–∫–∏)
        if hasattr(self.logger, 'report_single_value'):
            self.logger.report_single_value("model_size_bytes", model.get_model_size())
            if retriever is not None:
                self.logger.report_single_value("retriever_index_size", retriever.get_index_size())
            # Log dataset stats
            for key, value in dataset.get_dataset_stats().items():
                self.logger.report_single_value(f"dataset_{key}", value)
        else:
            # Fallback –¥–ª—è —Ä–µ–∂–∏–º–∞ –±–µ–∑ ClearML
            self.logger.info(f"Model size: {model.get_model_size()}")
            if retriever is not None:
                self.logger.info(f"Retriever index size: {retriever.get_index_size()}")
            for key, value in dataset.get_dataset_stats().items():
                self.logger.info(f"Dataset {key}: {value}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
        self.logger.report_text("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏...")
        start_time = time.time()
        
        metrics = self._evaluate(model, retriever, dataset)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        if hasattr(self.logger, 'report_single_value'):
            self.logger.report_single_value("duration_seconds", duration)
        self.logger.report_text(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞—Ö –≤ –º–µ—Ç—Ä–∏–∫–∏
        metrics['duration_seconds'] = duration
        metrics['model_size_bytes'] = model.get_model_size()
        metrics['model_size_mb'] = model.get_model_size() / (1024 * 1024)
        
        if retriever is not None:
            metrics['retriever_index_size_bytes'] = retriever.get_index_size()
            metrics['retriever_index_size_mb'] = retriever.get_index_size() / (1024 * 1024)
        else:
            metrics['retriever_index_size_bytes'] = 0
            metrics['retriever_index_size_mb'] = 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_results(metrics)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞
        self.memory_tracker.log_memory("system", "experiment_end")
        self.memory_tracker.save_log()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∏–∫–æ–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –≤ –º–µ—Ç—Ä–∏–∫–∏
        if self.memory_tracker.peak_stats:
            peak_memory = self.memory_tracker.peak_stats.to_dict()
            metrics.update(peak_memory)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∏–∫–æ–≤—É—é –ø–∞–º—è—Ç—å
            self.logger.report_text("üíæ –ü–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:")
            self.logger.report_text(f"  CPU RAM: {peak_memory['cpu_ram_used_mb']:.2f} MB")
            if peak_memory['gpu_ram_peak_mb'] > 0:
                self.logger.report_text(f"  GPU RAM (peak): {peak_memory['gpu_ram_peak_mb']:.2f} MB")
                self.logger.report_text(f"  GPU RAM (reserved): {peak_memory['reserved_gpu_ram_mb']:.2f} MB")
        
        # –ü–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        self._save_results(metrics)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
        self.predictions_tracker.save_predictions()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        self.logger.report_text("‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        self.logger.report_text(f"üìà –¢–æ–∫–µ–Ω-—Ä–µ–∫–æ–ª–ª: {metrics.get('token_recall', 0):.4f}")
        self.logger.report_text(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {metrics.get('num_examples', 0)}")
        self.logger.report_text(f"üì¶ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {metrics.get('model_size_mb', 0):.2f} MB")
        self.logger.report_text(f"üîç –†–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞ RAG: {metrics.get('retriever_index_size_mb', 0):.2f} MB")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        if not self.use_clearml and hasattr(self, 'local_logger'):
            self.local_logger.save_all(self.config.name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            results_file = self.config.output_dir / "results.json"
            if results_file.exists():
                self.local_logger.save_artifact("experiment_results", results_file)
            
            predictions_file = self.config.output_dir / "predictions.json"
            if predictions_file.exists():
                self.local_logger.save_artifact("model_predictions", predictions_file)
            
            memory_file = self.config.output_dir / "memory_usage.json"
            if memory_file.exists():
                self.local_logger.save_artifact("memory_usage", memory_file)
        
    def _get_context(self, item: DatasetItem, retriever: Optional[BaseRetriever]) -> List[str]:
        """Get context based on experiment mode."""
        if not self.config.use_retriever:
            if self.config.context_type == "none":
                return []
            elif self.config.context_type == "oracle":
                return [item.context] if item.context else []
        else:
            if not retriever:
                raise ValueError("Retriever is required for retriever_context mode")
            contexts = retriever.retrieve(
                item.question, 
                top_k=self.config.top_k
            )
            return [c.context for c in contexts if c.score >= self.config.min_score]

    def _evaluate(self, model: BaseModel, retriever: Optional[BaseRetriever], 
                 dataset: BaseDataset) -> Dict[str, float]:
        """Evaluate model performance using token recall metric."""
        recalls = []
        processed = 0
        logged_prompt_examples = 0  # Track how many prompt examples we've logged
        max_prompt_examples = 20  # Log first 20 prompt examples
        
        # Get total number of examples for progress tracking
        eval_data = list(dataset.get_eval_data())
        total_examples = len(eval_data)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ (–¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞)
        valid_examples = sum(1 for item in eval_data if item.answer)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        max_to_process = valid_examples
        if self.config.max_samples:
            max_to_process = min(valid_examples, self.config.max_samples)
        
        self.logger.info(f"üìä –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {max_to_process} (–∏–∑ {total_examples} –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ)")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        if HAS_TQDM:
            pbar = tqdm(
                total=max_to_process,
                desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤",
                unit="–ø—Ä–∏–º–µ—Ä",
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
            )
        else:
            pbar = None
        
        for item in eval_data:
            if not item.answer:  # Skip items without ground truth
                continue
            
            if self.config.max_samples and processed >= self.config.max_samples:
                break
                
            # Track memory for retrieval
            if retriever:
                self.memory_tracker.log_memory("retriever", "before_retrieve")
            
            # Get context based on experiment mode
            contexts = self._get_context(item, retriever)
            
            if retriever:
                self.memory_tracker.log_memory("retriever", "after_retrieve")
            
            # Track memory for model inference
            self.memory_tracker.log_memory("model", "before_generate")
            
            # Generate answer
            predicted_answer = model.generate(
                prompt=item.question,
                context=contexts,
                prompt_template=self.config.prompt_template
            )
            
            # Log prompt examples for first few items
            if logged_prompt_examples < max_prompt_examples and hasattr(model, 'last_prompt'):
                if hasattr(self.logger, 'report_text'):
                    self.logger.report_text(f"\nüí¨ –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ #{logged_prompt_examples + 1}:")
                    self.logger.report_text("```")
                    self.logger.report_text(model.last_prompt)
                    self.logger.report_text("```")
                    self.logger.report_text(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {predicted_answer}")
                    self.logger.report_text(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {item.answer}")
                logged_prompt_examples += 1
            
            self.memory_tracker.log_memory("model", "after_generate")
            
            # Calculate token recall
            recall = self.metric_calculator.calculate_recall(
                predicted=predicted_answer,
                ground_truth=item.answer
            )
            recalls.append(recall)
            
            # Track prediction
            self.predictions_tracker.add_prediction(
                question_id=item.metadata.get('id', str(processed)),
                question=item.question,
                predicted_answer=predicted_answer,
                ground_truth=item.answer,
                contexts=contexts,
                context_type=self.config.context_type,
                model_name=self.config.model_config.get('name', 'unknown'),
                token_recall=recall,
                metadata={
                    'dataset': self.config.dataset_config.get('name', 'unknown'),
                    **item.metadata
                },
                prompt=model.last_prompt if hasattr(model, 'last_prompt') else None
            )
            
            # Log individual example progress (—Å–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
            if hasattr(self.logger, 'report_scalar'):
                self.logger.report_scalar(
                    title="Training Progress",
                    series="token_recall",
                    value=recall,
                    iteration=processed
                )
            
            # Increment processed counter
            processed += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            if pbar:
                pbar.update(1)
                pbar.set_postfix({
                    'recall': f'{mean(recalls)*100:.2f}%' if recalls else '0%',
                    'processed': processed
                })
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ª–æ–≥ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
            log_interval = 10  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 –ø—Ä–∏–º–µ—Ä–æ–≤
            if processed % log_interval == 0 or processed == 1:
                avg_recall = mean(recalls) if recalls else 0.0
                progress_pct = (processed / max_to_process * 100) if max_to_process > 0 else 0
                self.logger.info(
                    f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{max_to_process} ({progress_pct:.1f}%) | "
                    f"–°—Ä–µ–¥–Ω–∏–π recall: {avg_recall*100:.2f}% | "
                    f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}"
                )
            
            # Clear memory periodically and log progress AFTER incrementing
            if processed % 100 == 0:
                self.memory_tracker.clear_memory()
                avg_recall = mean(recalls) if recalls else 0.0
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 100 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞–º—è—Ç–∏
                memory_stats = self.memory_tracker.get_current_memory_usage()
                gpu_info = f"GPU RAM: {memory_stats.gpu_ram_used:.1f} MB" if memory_stats.gpu_ram_used else "GPU RAM: N/A"
                self.logger.info(
                    f"üíæ –ü–∞–º—è—Ç—å: CPU RAM: {memory_stats.cpu_ram_used:.1f} MB | {gpu_info}"
                )
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        if pbar:
            pbar.close()
        
        # Calculate average recall
        avg_recall = mean(recalls) if recalls else 0.0
        
        return {
            "token_recall": avg_recall,
            "num_examples": len(recalls)
        }
    
    def _save_results(self, metrics: Dict[str, float]):
        """Save experiment results to disk and upload to ClearML."""
        results_file = self.config.output_dir / "results.json"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–∫–∞–ª—å–Ω–æ
        with open(results_file, "w") as f:
            json.dump(metrics, f, indent=2)
        
        if self.task is not None:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º output_uri –¥–ª—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ S3
            import os
            s3_output_uri = f"s3://clearml-artifacts/{self.config.name}"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –≤ ClearML
            self.task.upload_artifact(
                name="experiment_results",
                artifact_object=results_file,
                metadata={
                    "experiment_name": self.config.name,
                    "model": self.config.model_config.get('name', 'unknown'),
                    "dataset": self.config.dataset_config.get('name', 'unknown'),
                    "retriever": self.config.retriever_config.get('name', 'unknown'),
                    "timestamp": time.time()
                }
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
            predictions_file = self.config.output_dir / "predictions.json"
            if predictions_file.exists():
                self.task.upload_artifact(
                    name="model_predictions",
                    artifact_object=predictions_file,
                    metadata={
                        "experiment_name": self.config.name,
                        "num_predictions": len(self.predictions_tracker.predictions),
                        "timestamp": time.time()
                    }
                )
            
            # –¢–∞–∫–∂–µ –∑–∞–≥—Ä—É–∂–∞–µ–º memory usage –µ—Å–ª–∏ –µ—Å—Ç—å
            memory_file = self.config.output_dir / "memory_usage.json"
            if memory_file.exists():
                self.task.upload_artifact(
                    name="memory_usage",
                    artifact_object=memory_file,
                    metadata={
                        "experiment_name": self.config.name,
                        "timestamp": time.time()
                    }
                )
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–¥–∞–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
            cleanup_local = self.config.model_config.get('cleanup_local_artifacts', False)
            if cleanup_local:
                self.logger.info("üóëÔ∏è  –û—á–∏—Å—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ S3...")
                import os
                try:
                    if results_file.exists():
                        os.remove(results_file)
                    if predictions_file.exists():
                        os.remove(predictions_file)
                    if memory_file.exists():
                        os.remove(memory_file)
                    self.logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —É–¥–∞–ª–µ–Ω—ã (—Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ S3)")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã: {e}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ ClearML
            if hasattr(self.predictions_tracker, 'predictions') and self.predictions_tracker.predictions:
                log_predictions_to_clearml(self.logger, self.predictions_tracker.predictions)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ ClearML
            log_metrics_to_clearml(self.logger, metrics)
        else:
            # –†–µ–∂–∏–º –±–µ–∑ ClearML - –ª–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
            if hasattr(self, 'local_logger'):
                # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∫ single values
                for metric_name, value in metrics.items():
                    self.local_logger.report_single_value(metric_name, value)
                
                # –¢–∞–∫–∂–µ –ª–æ–≥–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ
                self.logger.info("üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:")
                for metric_name, value in metrics.items():
                    self.logger.info(f"  {metric_name}: {value:.4f}")
