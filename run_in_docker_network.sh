#!/bin/bash
# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ Docker —Å–µ—Ç—å
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π

set -e

SCRIPT_NAME="${1}"
if [ -z "$SCRIPT_NAME" ]; then
    echo "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: $0 <script.py> [args...]"
    exit 1
fi
shift

if [ ! -f "$SCRIPT_NAME" ]; then
    echo "‚ùå –§–∞–π–ª $SCRIPT_NAME –Ω–µ –Ω–∞–π–¥–µ–Ω"
    exit 1
fi

CLEARML_NETWORK="clearml_backend"

if ! docker network inspect "$CLEARML_NETWORK" > /dev/null 2>&1; then
    echo "‚ùå Docker —Å–µ—Ç—å $CLEARML_NETWORK –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    exit 1
fi

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ GPU
USE_GPU=false
if docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi > /dev/null 2>&1; then
    USE_GPU=true
    echo "üéØ GPU –¥–æ—Å—Ç—É–ø–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"
else
    echo "üíª GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ Docker, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU"
fi

echo "üöÄ –ó–∞–ø—É—Å–∫ $SCRIPT_NAME —á–µ—Ä–µ–∑ Docker —Å–µ—Ç—å $CLEARML_NETWORK"
echo "üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: clearml.conf.docker"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫–µ—à–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ö–æ—Å—Ç–µ (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
# –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è DOCKER_MODELS_CACHE
CACHE_DIR="${DOCKER_MODELS_CACHE:-/storage/docker-models}"
mkdir -p "$CACHE_DIR/huggingface"
mkdir -p "$CACHE_DIR/datasets"
echo "üíæ –ö–µ—à –º–æ–¥–µ–ª–µ–π: $CACHE_DIR"

# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É, –ø—Ä–∞–≤–∏–ª—å–Ω–æ —ç–∫—Ä–∞–Ω–∏—Ä—É—è
ARGS="$@"

# –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
# –ú–æ–Ω—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é –≤ ~/.clearml.conf
# –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
# –ú–æ–Ω—Ç–∏—Ä—É–µ–º –∫–µ—à –º–æ–¥–µ–ª–µ–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
DOCKER_ARGS="--rm --network $CLEARML_NETWORK"
if [ "$USE_GPU" = true ]; then
    DOCKER_ARGS="$DOCKER_ARGS --gpus all"
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–∑ —Å CUDA –¥–ª—è GPU
    BASE_IMAGE="pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime"
else
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫–∏–π –æ–±—Ä–∞–∑ –¥–ª—è CPU
    BASE_IMAGE="python:3.10-slim"
fi

docker run $DOCKER_ARGS \
    -v "$(pwd):/workspace" \
    -v "$(pwd)/clearml.conf.docker:/root/.clearml.conf:ro" \
    -v "$(pwd)/.env:/workspace/.env:ro" \
    -v "$CACHE_DIR/huggingface:/root/.cache/huggingface" \
    -v "$CACHE_DIR/datasets:/root/.cache/datasets" \
    -w /workspace \
    -e PYTHONPATH=/workspace \
    -e TRANSFORMERS_CACHE=/root/.cache/huggingface \
    -e HF_HOME=/root/.cache/huggingface \
    -e CLEARML_S3_ENDPOINT=http://minio:9000 \
    -e CLEARML_S3_BUCKET=clearml-artifacts \
    -e CLEARML_S3_ACCESS_KEY=minioadmin \
    -e CLEARML_S3_SECRET_KEY=minioadmin \
    -e CLEARML_S3_REGION=us-east-1 \
    "$BASE_IMAGE" \
    bash -c "
        if [ \"$USE_GPU\" = false ]; then
            pip install -q --no-cache-dir clearml boto3 python-dotenv requests omegaconf hydra-core torch transformers tqdm pandas
        else
            pip install -q --no-cache-dir clearml boto3 python-dotenv requests omegaconf hydra-core transformers tqdm pandas
        fi
        echo '‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ClearML —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ~/.clearml.conf'
        echo '‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è MinIO —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã'
        echo '‚úÖ –ö–µ—à –º–æ–¥–µ–ª–µ–π —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω: /root/.cache/huggingface'
        echo 'üíæ –ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∫–µ—à–∏—Ä–æ–≤–∞—Ç—å—Å—è –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏'
        python $SCRIPT_NAME $ARGS
    "

