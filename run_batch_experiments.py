#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º GPU –ø–∞–º—è—Ç—å—é.
–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (–≤—Å–µ –º–æ–¥–µ–ª–∏ √ó –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã)
    poetry run python run_batch_experiments.py
    
    # –ó–∞–ø—É—Å–∫ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
    poetry run python run_batch_experiments.py --models qwen_0.6b qwen_1.7b --datasets local_simple_qa
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
    poetry run python run_batch_experiments.py --max-parallel 2
    
    # –ë–µ–∑ ClearML –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    poetry run python run_batch_experiments.py --no-clearml
    
    # –ò–ª–∏ –µ—Å–ª–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ –æ–∫—Ä—É–∂–µ–Ω–∏–µ Poetry (poetry shell):
    python run_batch_experiments.py

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ GPU —á–µ—Ä–µ–∑ nvidia-smi
    - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Å —É—á–µ—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3 –ø–æ–ø—ã—Ç–∫–∏)
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ ClearML
    - –û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
"""

import subprocess
import json
import time
import logging
import threading
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from queue import Queue
from omegaconf import OmegaConf
import yaml

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('batch_experiments.log')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class ExperimentTask:
    """–ó–∞–¥–∞—á–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    model: str
    dataset: str
    experiment_mode: str
    estimated_memory_gb: float
    retry_count: int = 0
    max_retries: int = 3
    status: str = "pending"  # pending, running, completed, failed
    task_id: str = ""  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
    gpu_id: Optional[int] = None  # GPU, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∑–∞–¥–∞—á–∞
    failure_reason: Optional[str] = None  # –ü—Ä–∏—á–∏–Ω–∞ –Ω–µ—É–¥–∞—á–∏ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)


class GPUMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏."""
    
    def __init__(self):
        self.gpu_count = self._get_gpu_count()
        self.gpu_locks = [threading.Lock() for _ in range(self.gpu_count)]  # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π GPU
        self.gpu_reservations = {}  # –°–ª–æ–≤–∞—Ä—å: gpu_id -> task_id
        logger.info(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {self.gpu_count}")
    
    def _get_gpu_count(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU."""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--list-gpus'],
                capture_output=True,
                text=True,
                check=True
            )
            return len(result.stdout.strip().split('\n'))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {e}")
            return 0
    
    def get_free_memory_per_gpu(self) -> List[float]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–±–æ–¥–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–π GPU –≤ GB.
        –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –≤—Å–µ–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (–≤–∫–ª—é—á–∞—è –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π).
        """
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                check=True
            )
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ MB –≤ GB
            free_memories = [float(line.strip()) / 1024.0 for line in result.stdout.strip().split('\n') if line.strip()]
            return free_memories
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU –ø–∞–º—è—Ç–∏: {e}")
            return [0.0] * self.gpu_count
    
    def is_gpu_actually_free(self, gpu_id: int, required_memory_gb: float, reserved_memory_gb: float = 2.0) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ GPU —Å–≤–æ–±–æ–¥–Ω–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
        –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (–≤–∫–ª—é—á–∞—è –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π).
        
        Args:
            gpu_id: ID GPU –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            required_memory_gb: –¢—Ä–µ–±—É–µ–º–∞—è –ø–∞–º—è—Ç—å –≤ GB
            reserved_memory_gb: –†–µ–∑–µ—Ä–≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã
        
        Returns:
            True –µ—Å–ª–∏ GPU –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–≤–æ–±–æ–¥–Ω–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPU –Ω–µ –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞—à–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º
        if gpu_id in self.gpu_reservations:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU —á–µ—Ä–µ–∑ nvidia-smi
        # –≠—Ç–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        free_memories = self.get_free_memory_per_gpu()
        
        if gpu_id >= len(free_memories):
            return False
        
        free_memory = free_memories[gpu_id]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏
        if free_memory < (required_memory_gb + reserved_memory_gb):
            logger.debug(f"GPU {gpu_id} –∑–∞–Ω—è—Ç–∞ –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏: —Å–≤–æ–±–æ–¥–Ω–æ {free_memory:.1f}GB, —Ç—Ä–µ–±—É–µ—Ç—Å—è {required_memory_gb + reserved_memory_gb:.1f}GB")
            return False
        
        return True
    
    def get_total_memory_per_gpu(self) -> List[float]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é –ø–∞–º—è—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–π GPU –≤ GB."""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                check=True
            )
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ MB –≤ GB
            total_memories = [float(line.strip()) / 1024.0 for line in result.stdout.strip().split('\n') if line.strip()]
            return total_memories
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU –ø–∞–º—è—Ç–∏: {e}")
            return [0.0] * self.gpu_count
    
    def find_available_gpu(self, required_memory_gb: float, reserved_memory_gb: float = 2.0, task_id: Optional[str] = None, enforce_exclusive: bool = True) -> Optional[int]:
        """
        –ù–∞–π—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—É—é GPU —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏.
        –†–µ–∑–µ—Ä–≤–∏—Ä—É–µ—Ç GPU –¥–ª—è –∑–∞–¥–∞—á–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–æ—Ä–∫–µ—Ä–∞–º–∏.
        
        –í–ê–ñ–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é enforce_exclusive=True –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–¥–Ω–∞ GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        –º–æ—â–Ω–æ—Å—Ç–∏ GPU - nvidia-smi –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—â–µ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –≤—Å–µ–π GPU, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞.
        
        Args:
            required_memory_gb: –¢—Ä–µ–±—É–µ–º–∞—è –ø–∞–º—è—Ç—å –≤ GB
            reserved_memory_gb: –†–µ–∑–µ—Ä–≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2GB)
            task_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∏—è
            enforce_exclusive: –ï—Å–ª–∏ True, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
                              (–æ–¥–Ω–∞ GPU = –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç). –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏.
        
        Returns:
            –ò–Ω–¥–µ–∫—Å GPU –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é GPU
        for gpu_idx in range(self.gpu_count):
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPU –Ω–µ –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞—à–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–¥–Ω–∞ GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–º
            if gpu_idx in self.gpu_reservations:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (—É—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
            if not self.is_gpu_actually_free(gpu_idx, required_memory_gb, reserved_memory_gb):
                continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ enforce_exclusive=True, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ GPU –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–≤–æ–±–æ–¥–Ω–∞
            # (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å)
            if enforce_exclusive:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞–º—è—Ç—å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É GPU
                free_memories = self.get_free_memory_per_gpu()
                total_memories = self.get_total_memory_per_gpu()
                
                if gpu_idx < len(free_memories) and gpu_idx < len(total_memories):
                    free_mem = free_memories[gpu_idx]
                    total_mem = total_memories[gpu_idx]
                    used_mem = total_mem - free_mem
                    
                    # –ï—Å–ª–∏ GPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –±–æ–ª—å—à–µ 5% –ø–∞–º—è—Ç–∏),
                    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—ë –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç–∏
                    if used_mem > total_mem * 0.05:  # –ë–æ–ª–µ–µ 5% –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                        logger.debug(f"‚è≠Ô∏è  GPU {gpu_idx} –ø—Ä–æ–ø—É—â–µ–Ω–∞: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ ({used_mem:.1f}GB/{total_mem:.1f}GB)")
                        continue
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å GPU
            if self.gpu_locks[gpu_idx].acquire(blocking=False):
                try:
                    # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (double-checked locking)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑, —á—Ç–æ GPU –Ω–µ –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Ä–µ–∞–ª—å–Ω–æ —Å–≤–æ–±–æ–¥–Ω–∞
                    if gpu_idx not in self.gpu_reservations and self.is_gpu_actually_free(gpu_idx, required_memory_gb, reserved_memory_gb):
                        if task_id:
                            self.gpu_reservations[gpu_idx] = task_id
                        logger.debug(f"‚úÖ GPU {gpu_idx} –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}")
                        return gpu_idx
                finally:
                    self.gpu_locks[gpu_idx].release()
        
        return None
    
    def release_gpu(self, gpu_id: int, task_id: Optional[str] = None):
        """
        –û—Å–≤–æ–±–æ–¥–∏—Ç—å GPU –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.
        
        Args:
            gpu_id: ID GPU –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è
            task_id: ID –∑–∞–¥–∞—á–∏ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
        """
        if gpu_id in self.gpu_reservations:
            if task_id is None or self.gpu_reservations[gpu_id] == task_id:
                del self.gpu_reservations[gpu_id]
                logger.debug(f"üîì GPU {gpu_id} –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞")


class BatchExperimentRunner:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞–∫–µ—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
    
    def __init__(
        self,
        models: List[str],
        datasets: List[str],
        experiment_mode: str = "no_context",
        max_parallel: Optional[int] = None,
        retry_count: int = 3,
        use_clearml: bool = True
    ):
        self.models = models
        self.datasets = datasets
        self.experiment_mode = experiment_mode
        self.max_retries = retry_count
        self.use_clearml = use_clearml
        
        self.gpu_monitor = GPUMonitor()
        self.gpu_count = self.gpu_monitor.gpu_count
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU (—á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ GPU –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
        # –í–ê–ñ–ù–û: –î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏ GPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        # max_parallel <= gpu_count, —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–º
        self.max_parallel = max_parallel or self.gpu_count
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏
        # –ï—Å–ª–∏ max_parallel > gpu_count, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –æ –ø—Ä–æ–±–ª–µ–º–µ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–æ—â–Ω–æ—Å—Ç–∏
        if self.max_parallel > self.gpu_count:
            logger.warning(f"‚ö†Ô∏è  max_parallel ({self.max_parallel}) –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ GPU ({self.gpu_count})")
            logger.warning(f"üí° –ù–µ—Å–∫–æ–ª—å–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É GPU –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ")
            logger.warning(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ—â–Ω–æ—Å—Ç—å GPU –±—É–¥–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –æ–±—â–∞—è –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–∞ GPU!")
            logger.warning(f"üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å --max-parallel {self.gpu_count} –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏")
        elif self.max_parallel < self.gpu_count:
            logger.warning(f"‚ö†Ô∏è  max_parallel ({self.max_parallel}) –º–µ–Ω—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ GPU ({self.gpu_count})")
            logger.warning(f"üí° –ù–µ –≤—Å–µ GPU –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å --max-parallel {self.gpu_count}")
        else:
            logger.info(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö {self.gpu_count} GPU –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ")
            logger.info(f"‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (–æ–¥–Ω–∞ GPU = –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç)")
            logger.info(f"‚úÖ –ú–æ—â–Ω–æ—Å—Ç—å GPU –±—É–¥–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
        
        # –û—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á
        self.task_queue = Queue()
        self.running_tasks: Dict[int, ExperimentTask] = {}
        self.completed_tasks: List[ExperimentTask] = []
        self.failed_tasks: List[ExperimentTask] = []
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.lock = threading.Lock()
        
        # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π (–≤ GB)
        self.model_memory_estimates = self._load_model_memory_estimates()
        
        # ClearML –∑–∞–¥–∞—á–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.clearml_task = None
        self.clearml_logger = None
        if self.use_clearml:
            self._setup_clearml()
    
    def _setup_clearml(self):
        """–ù–∞—Å—Ç—Ä–æ–∏—Ç—å ClearML –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
        try:
            from clearml import Task, Logger
            from dotenv import load_dotenv
            import os
            
            load_dotenv()
            
            self.clearml_task = Task.create(
                project_name="slm-experiments",
                task_name=f"batch_experiments_{self.experiment_mode}",
                task_type=Task.TaskTypes.custom
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            self.clearml_task.connect({
                "models": self.models,
                "datasets": self.datasets,
                "experiment_mode": self.experiment_mode,
                "max_parallel": self.max_parallel,
                "gpu_count": self.gpu_count,
                "retry_count": self.max_retries
            })
            
            self.clearml_logger = Logger.current_logger()
            logger.info("‚úÖ ClearML –∑–∞–¥–∞—á–∞ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å ClearML: {e}")
            self.use_clearml = False
    
    def _load_model_memory_estimates(self) -> Dict[str, float]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤."""
        estimates = {}
        configs_dir = Path("configs/model")
        
        for model_file in configs_dir.glob("*.yaml"):
            try:
                with open(model_file, 'r') as f:
                    config = yaml.safe_load(f)
                    model_name = config.get('name', model_file.stem)
                    model_size = config.get('model_size', '0B')
                    
                    # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "0.6B", "1.7B", "4B")
                    size_str = model_size.replace('B', '').strip()
                    try:
                        size_value = float(size_str)
                        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚âà 2GB –ø–∞–º—è—Ç–∏ (float16)
                        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ –±—É—Ñ–µ—Ä–æ–≤
                        estimated_gb = size_value * 2.5  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å –∑–∞–ø–∞—Å–æ–º
                        estimates[model_name] = estimated_gb
                        logger.info(f"üìä –ú–æ–¥–µ–ª—å {model_name}: –æ—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ ~{estimated_gb:.1f}GB")
                    except ValueError:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        if '0.6' in size_str or '135m' in model_name.lower():
                            estimates[model_name] = 2.0
                        elif '1.7' in size_str or '360m' in model_name.lower():
                            estimates[model_name] = 4.0
                        elif '4' in size_str:
                            estimates[model_name] = 9.0
                        else:
                            estimates[model_name] = 3.0
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è {model_file}: {e}")
        
        return estimates
    
    def generate_tasks(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∑–∞–¥–∞—á–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
        tasks = []
        
        for model in self.models:
            for dataset in self.datasets:
                estimated_memory = self.model_memory_estimates.get(model, 3.0)
                task_id = f"{model}_{dataset}_{self.experiment_mode}"
                task = ExperimentTask(
                    model=model,
                    dataset=dataset,
                    experiment_mode=self.experiment_mode,
                    estimated_memory_gb=estimated_memory,
                    max_retries=self.max_retries,
                    task_id=task_id
                )
                tasks.append(task)
                self.task_queue.put(task)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –ø–∞–º—è—Ç–∏ (–º–∞–ª–µ–Ω—å–∫–∏–µ –ø–µ—Ä–≤—ã–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
        tasks_sorted = sorted(tasks, key=lambda t: t.estimated_memory_gb)
        logger.info(f"üìã –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(tasks)} –∑–∞–¥–∞—á —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞–º—è—Ç–∏:")
        for task in tasks_sorted:
            logger.info(f"   {task.model} √ó {task.dataset}: ~{task.estimated_memory_gb:.1f}GB")
        
        return len(tasks)
    
    def run_experiment(self, task: ExperimentTask, gpu_id: int) -> bool:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ.
        
        Args:
            task: –ó–∞–¥–∞—á–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            gpu_id: ID GPU –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        import os
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Docker
        workspace_dir = os.getcwd()
        cache_dir = os.path.expanduser("~/.cache/docker-models")
        image_name = "slm-experiments:latest"
        network_name = "clearml_backend"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Docker –æ–±—Ä–∞–∑ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            result = subprocess.run(
                ["docker", "image", "inspect", image_name],
                capture_output=True,
                check=True
            )
        except subprocess.CalledProcessError:
            logger.error(f"‚ùå Docker –æ–±—Ä–∞–∑ {image_name} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            logger.error(f"üí° –°–æ–±–µ—Ä–∏—Ç–µ –æ–±—Ä–∞–∑ –∫–æ–º–∞–Ω–¥–æ–π: ./build_docker_image.sh")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Docker —Å–µ—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            subprocess.run(
                ["docker", "network", "inspect", network_name],
                capture_output=True,
                check=True
            )
        except subprocess.CalledProcessError:
            logger.error(f"‚ùå Docker —Å–µ—Ç—å {network_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Docker
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º --gpus device=N –¥–ª—è –≤—ã–±–æ—Ä–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π GPU
        # –í—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ run_experiment_fast.sh, –Ω–æ —Å –≤—ã–±–æ—Ä–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π GPU
        docker_cmd = [
            "docker", "run", "--rm",
            "--network", network_name,
            "--gpus", f"device={gpu_id}",  # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é GPU
            "-v", f"{workspace_dir}:/workspace",
            "-v", f"{workspace_dir}/clearml.conf.docker:/root/.clearml.conf:ro",
            "-v", f"{workspace_dir}/.env:/workspace/.env:ro",
            "-v", f"{cache_dir}/huggingface:/root/.cache/huggingface",
            "-v", f"{cache_dir}/datasets:/root/.cache/datasets",
            "-w", "/workspace",
            "-e", "PYTHONPATH=/workspace",
            "-e", "TRANSFORMERS_CACHE=/root/.cache/huggingface",
            "-e", "HF_HOME=/root/.cache/huggingface",
            "-e", "CLEARML_S3_ENDPOINT=http://minio:9000",
            "-e", "CLEARML_S3_BUCKET=clearml-artifacts",
            "-e", "CLEARML_S3_ACCESS_KEY=minioadmin",
            "-e", "CLEARML_S3_SECRET_KEY=minioadmin",
            "-e", "CLEARML_S3_REGION=us-east-1",
            image_name,
            "bash", "-c",
            f"python run_experiment_simple.py model={task.model} dataset={task.dataset} experiment_mode={task.experiment_mode}"
        ]
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –≤ Docker: {task.model} √ó {task.dataset} –Ω–∞ GPU {gpu_id}")
        logger.debug(f"   –ü–æ–ª–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {' '.join(docker_cmd)}")
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
            process = subprocess.Popen(
                docker_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            def log_output(pipe, prefix):
                for line in iter(pipe.readline, ''):
                    if line:
                        logger.info(f"[GPU{gpu_id}][{task.model}√ó{task.dataset}] {prefix}: {line.rstrip()}")
            
            stdout_thread = threading.Thread(target=log_output, args=(process.stdout, "STDOUT"))
            stderr_thread = threading.Thread(target=log_output, args=(process.stderr, "STDERR"))
            stdout_thread.daemon = True
            stderr_thread.daemon = True
            stdout_thread.start()
            stderr_thread.start()
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            return_code = process.wait()
            
            stdout_thread.join(timeout=1)
            stderr_thread.join(timeout=1)
            
            if return_code == 0:
                logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω: {task.model} √ó {task.dataset} –Ω–∞ GPU {gpu_id}")
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ (–∫–æ–¥ {return_code}): {task.model} √ó {task.dataset} –Ω–∞ GPU {gpu_id}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def worker_thread(self, worker_id: int):
        """–ü–æ—Ç–æ–∫-–≤–æ—Ä–∫–µ—Ä –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
        logger.info(f"üîÑ –í–æ—Ä–∫–µ—Ä {worker_id} –∑–∞–ø—É—â–µ–Ω")
        
        while True:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                task = self.task_queue.get(timeout=1)
                
                if task is None:  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    break
                
                # –ò—â–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é GPU
                # –ë—É–¥–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ GPU –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –º–µ—Ä–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU
                gpu_id = None
                wait_start = time.time()
                last_log_time = wait_start
                check_interval = 10  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
                
                logger.info(f"üîç –í–æ—Ä–∫–µ—Ä {worker_id} –∏—â–µ—Ç —Å–≤–æ–±–æ–¥–Ω—É—é GPU –¥–ª—è {task.model} √ó {task.dataset} (—Ç—Ä–µ–±—É–µ—Ç—Å—è ~{task.estimated_memory_gb:.1f}GB)")
                
                while gpu_id is None:
                    gpu_id = self.gpu_monitor.find_available_gpu(
                        task.estimated_memory_gb,
                        reserved_memory_gb=2.0,
                        task_id=task.task_id
                    )
                    
                    if gpu_id is None:
                        elapsed = time.time() - wait_start
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 60 —Å–µ–∫—É–Ω–¥
                        if time.time() - last_log_time >= 60:
                            logger.info(
                                f"‚è≥ –í–æ—Ä–∫–µ—Ä {worker_id} –∂–¥–µ—Ç —Å–≤–æ–±–æ–¥–Ω—É—é GPU –¥–ª—è {task.model} √ó {task.dataset} "
                                f"(–ø—Ä–æ—à–ª–æ {elapsed/60:.1f} –º–∏–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è ~{task.estimated_memory_gb:.1f}GB). "
                                f"–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—é –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ GPU..."
                            )
                            last_log_time = time.time()
                        
                        # –ñ–¥–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞
                        time.sleep(check_interval)
                
                if gpu_id is not None:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –≤ –∑–∞–¥–∞—á–µ
                    task.gpu_id = gpu_id
                    
                    # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω—è—é—â—É—é—Å—è
                    with self.lock:
                        task.status = "running"
                        self.running_tasks[worker_id] = task
                    
                    logger.info(f"üéØ –í–æ—Ä–∫–µ—Ä {worker_id}: {task.model} √ó {task.dataset} –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ GPU {gpu_id}")
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
                    success = self.run_experiment(task, gpu_id)
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU
                    self.gpu_monitor.release_gpu(gpu_id, task.task_id)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    with self.lock:
                        if success:
                            task.status = "completed"
                            self.completed_tasks.append(task)
                        else:
                            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ
                            if task.retry_count < task.max_retries:
                                task.retry_count += 1
                                task.status = "pending"
                                task.gpu_id = None  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º GPU –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
                                logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä {task.retry_count}/{task.max_retries}: {task.model} √ó {task.dataset}")
                                self.task_queue.put(task)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
                            else:
                                task.status = "failed"
                                self.failed_tasks.append(task)
                                logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫: {task.model} √ó {task.dataset}")
                        
                        del self.running_tasks[worker_id]
                
                self.task_queue.task_done()
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –≤–æ—Ä–∫–µ—Ä–µ {worker_id}: {e}")
                import traceback
                logger.error(traceback.format_exc())
    
    def run(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã."""
        total_tasks = self.generate_tasks()
        
        if total_tasks == 0:
            logger.error("‚ùå –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
            return
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ {total_tasks} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
        logger.info(f"üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {self.max_parallel}")
        logger.info(f"üéØ –î–æ—Å—Ç—É–ø–Ω–æ GPU: {self.gpu_count}")
        logger.info(f"üí° –í—Å–µ {self.gpu_count} GPU –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∑–∞–¥–∞—á")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä—ã
        workers = []
        for i in range(self.max_parallel):
            worker = threading.Thread(target=self.worker_thread, args=(i,))
            worker.daemon = True
            worker.start()
            workers.append(worker)
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        last_report_time = time.time()
        while not self.task_queue.empty() or self.running_tasks:
            time.sleep(10)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if time.time() - last_report_time > 60:  # –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                completed = len(self.completed_tasks)
                failed = len(self.failed_tasks)
                running = len(self.running_tasks)
                remaining = self.task_queue.qsize()
                total = completed + failed + running + remaining
                progress_pct = (completed / total * 100) if total > 0 else 0
                
                logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: –∑–∞–≤–µ—Ä—à–µ–Ω–æ {completed}/{total} ({progress_pct:.1f}%), "
                          f"–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è {running}, –æ—Å—Ç–∞–ª–æ—Å—å {remaining}, –æ—à–∏–±–æ–∫ {failed}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –≤ ClearML
                if self.clearml_logger:
                    self.clearml_logger.report_scalar(
                        title="Batch Progress",
                        series="completed",
                        value=completed,
                        iteration=int(time.time())
                    )
                    self.clearml_logger.report_scalar(
                        title="Batch Progress",
                        series="failed",
                        value=failed,
                        iteration=int(time.time())
                    )
                    self.clearml_logger.report_scalar(
                        title="Batch Progress",
                        series="running",
                        value=running,
                        iteration=int(time.time())
                    )
                    self.clearml_logger.report_scalar(
                        title="Batch Progress",
                        series="remaining",
                        value=remaining,
                        iteration=int(time.time())
                    )
                    self.clearml_logger.report_scalar(
                        title="Batch Progress",
                        series="progress_percent",
                        value=progress_pct,
                        iteration=int(time.time())
                    )
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ GPU
                free_memories = self.gpu_monitor.get_free_memory_per_gpu()
                total_memories = self.gpu_monitor.get_total_memory_per_gpu()
                
                logger.info(f"üíª –°–æ—Å—Ç–æ—è–Ω–∏–µ GPU:")
                for gpu_idx, (free_mem, total_mem) in enumerate(zip(free_memories, total_memories)):
                    used_mem = total_mem - free_mem
                    usage_pct = (used_mem / total_mem * 100) if total_mem > 0 else 0
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∞—è –∑–∞–¥–∞—á–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç—É GPU
                    running_task_info = ""
                    for worker_id, running_task in self.running_tasks.items():
                        if running_task.gpu_id == gpu_idx:
                            running_task_info = f" [{running_task.model}√ó{running_task.dataset}]"
                            break
                    
                    status = "üü¢ —Å–≤–æ–±–æ–¥–Ω–∞" if gpu_idx not in self.gpu_monitor.gpu_reservations else "üî¥ –∑–∞–Ω—è—Ç–∞"
                    logger.info(f"   GPU {gpu_idx}: {status} | –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {used_mem:.1f}GB/{total_mem:.1f}GB ({usage_pct:.1f}%){running_task_info}")
                    
                    if self.clearml_logger:
                        self.clearml_logger.report_scalar(
                            title="GPU Memory",
                            series=f"gpu_{gpu_idx}_free_gb",
                            value=free_mem,
                            iteration=int(time.time())
                        )
                        self.clearml_logger.report_scalar(
                            title="GPU Memory",
                            series=f"gpu_{gpu_idx}_used_gb",
                            value=used_mem,
                            iteration=int(time.time())
                        )
                        self.clearml_logger.report_scalar(
                            title="GPU Memory",
                            series=f"gpu_{gpu_idx}_usage_pct",
                            value=usage_pct,
                            iteration=int(time.time())
                        )
                
                last_report_time = time.time()
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
        for _ in workers:
            self.task_queue.put(None)  # –°–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        
        for worker in workers:
            worker.join(timeout=10)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        logger.info("=" * 80)
        logger.info("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        logger.info("=" * 80)
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.completed_tasks)}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {len(self.failed_tasks)}")
        logger.info(f"üìã –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {total_tasks}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ ClearML
        if self.clearml_logger:
            self.clearml_logger.report_text("=" * 80)
            self.clearml_logger.report_text("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–ê–ö–ï–¢–ù–´–• –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
            self.clearml_logger.report_text("=" * 80)
            self.clearml_logger.report_text(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(self.completed_tasks)}")
            self.clearml_logger.report_text(f"‚ùå –û—à–∏–±–æ–∫: {len(self.failed_tasks)}")
            self.clearml_logger.report_text(f"üìã –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {total_tasks}")
            
            if self.completed_tasks:
                self.clearml_logger.report_text("\n‚úÖ –£—Å–ø–µ—à–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:")
                for task in self.completed_tasks:
                    self.clearml_logger.report_text(f"   {task.model} √ó {task.dataset}")
        
        if self.failed_tasks:
            logger.info("\n‚ùå –ù–µ—É–¥–∞—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:")
            for task in self.failed_tasks:
                reason = f" ({task.failure_reason})" if task.failure_reason else ""
                logger.info(f"   {task.model} √ó {task.dataset} (–ø–æ–ø—ã—Ç–æ–∫: {task.retry_count}){reason}")
            
            if self.clearml_logger:
                self.clearml_logger.report_text("\n‚ùå –ù–µ—É–¥–∞—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:")
                for task in self.failed_tasks:
                    reason = f" ({task.failure_reason})" if task.failure_reason else ""
                    self.clearml_logger.report_text(f"   {task.model} √ó {task.dataset} (–ø–æ–ø—ã—Ç–æ–∫: {task.retry_count}){reason}")
        
        logger.info("=" * 80)
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º ClearML –∑–∞–¥–∞—á—É
        if self.clearml_task:
            self.clearml_task.close()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--models', nargs='+', default=None,
                        help='–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ –∏–∑ configs/model/)')
    parser.add_argument('--datasets', nargs='+', default=None,
                        help='–°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é local_nq, local_simple_qa)')
    parser.add_argument('--experiment-mode', default='no_context',
                        help='–†–µ–∂–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: no_context)')
    parser.add_argument('--max-parallel', type=int, default=None,
                        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--retry-count', type=int, default=3,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)')
    parser.add_argument('--no-clearml', action='store_true',
                        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ ClearML')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
    if args.models is None:
        models_dir = Path("configs/model")
        models = sorted([f.stem for f in models_dir.glob("*.yaml")])
        logger.info(f"üì¶ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: {models}")
        logger.info(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
    else:
        models = args.models
        logger.info(f"üì¶ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —É–∫–∞–∑–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {models}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
    if args.datasets is None:
        datasets_dir = Path("configs/dataset")
        datasets = sorted([f.stem for f in datasets_dir.glob("*.yaml")])
        logger.info(f"üìä –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {datasets}")
        logger.info(f"   –í—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {len(datasets)}")
    else:
        datasets = args.datasets
        logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —É–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã: {datasets}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    total_experiments = len(models) * len(datasets)
    logger.info(f"üéØ –í—Å–µ–≥–æ –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {total_experiments} ({len(models)} –º–æ–¥–µ–ª–µ–π √ó {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)")
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    runner = BatchExperimentRunner(
        models=models,
        datasets=datasets,
        experiment_mode=args.experiment_mode,
        max_parallel=args.max_parallel,
        retry_count=args.retry_count,
        use_clearml=not args.no_clearml
    )
    
    runner.run()


if __name__ == "__main__":
    import os
    main()

